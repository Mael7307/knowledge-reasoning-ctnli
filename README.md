# üß† The knowledge-reasoning dissociation: Fundamental limitations of llms in clinical natural language inference

This repository provides a unified framework for evaluating Large Language Models (LLMs) on cognitive gap tasks, including natural language inference (NLI) and factual correctness evaluation.

We implement a modular pipeline for running experiments across multiple LLM providers (OpenAI, Azure OpenAI, Google Gemini, Ollama) with support for different prompting strategies and comprehensive evaluation metrics.

---

## üìã Table of Contents

- [Features](#features)
- [Installation](#installation)
- [Usage](#usage)
  - [Running Experiments](#running-experiments)
  - [Evaluating Results](#evaluating-results)
- [Project Structure](#project-structure)
- [Configuration](#configuration)
- [Data Format](#data-format)
- [Reproducibility](#reproducibility)
- [Citation](#citation)

---

## ‚ú® Features

- ‚úÖ Unified command-line interface for experiments and evaluation
- ‚úÖ Support for multiple LLM providers: OpenAI, Azure OpenAI, Google Gemini, and Ollama
- ‚úÖ Two task types: Natural Language Inference (NLI) and Factual Correctness
- ‚úÖ Multiple prompting strategies: Direct and Chain-of-Thought (CoT)
- ‚úÖ Comprehensive evaluation metrics: Accuracy and F1 score
- ‚úÖ Flexible output formats: Table, JSON, and LaTeX
- ‚úÖ Modular, extensible architecture for easy customization

---

## üõ†Ô∏è Installation

### 1. Clone this repository

```bash
git clone <repository-url>
cd CogGap
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Set up configuration

Copy the example configuration file and add your API keys:

```bash
cp config/config.yaml.example config/config.yaml
```

Edit `config/config.yaml` with your API credentials:

```yaml
gemini:
  api_key: "your-gemini-api-key"

lunar-deepseek-r1:
  api_key: "your-azure-api-key"
  version: "2024-02-15-preview"
  endpoint: "https://your-endpoint.openai.azure.com/"

openai:
  api_key: "your-openai-api-key"
```

**Note:** API keys can also be provided via command-line arguments or environment variables. See [Configuration](#configuration) for details.

---

## üöÄ Usage

### üî¨ Running Experiments

Run experiments across different models, tasks, and prompting strategies:

```bash
python scripts/run_experiment.py \
    --model-type azure_openai \
    --model-name deepseek-r1 \
    --task-type nli \
    --prompt-type cot \
    --data-dir data/main_task \
    --output-dir results/main_task \
    --input-files causal.json comp_ground.json epistemic.json risk.json \
    --num-runs 10
```

#### Model Types

- `openai`: OpenAI API models (GPT-4o, o3, GPT-4o-mini)
- `azure_openai`: Azure OpenAI models (deepseek-r1, lunar-gpt-4o, etc.)
- `gemini`: Google Gemini models (gemini-2.5-pro)
- `ollama`: Local Ollama models (llama3.2, etc.)

#### Task Types

- `nli`: Natural Language Inference ‚Äî predict relationship between premise and statement (entailment/neutral/contradiction)
- `factual`: Factual Correctness ‚Äî determine if statement is factually correct (True/False)

#### Prompt Types

- `direct`: Direct prompting without reasoning steps
- `cot`: Chain-of-Thought prompting with explicit reasoning

#### Examples

**NLI task with OpenAI:**
```bash
python scripts/run_experiment.py \
    --model-type openai \
    --model-name gpt-4o \
    --task-type nli \
    --prompt-type direct \
    --data-dir data/main_task \
    --output-dir results/main_task \
    --input-files causal.json
```

**Factual task with Gemini:**
```bash
python scripts/run_experiment.py \
    --model-type gemini \
    --model-name gemini-2.5-pro \
    --task-type factual \
    --prompt-type cot \
    --data-dir data/ctrl_GKMRV \
    --output-dir results/ctrl_GKMRV \
    --input-files comp_ground.json
```

**Local model with Ollama:**
```bash
python scripts/run_experiment.py \
    --model-type ollama \
    --model-name llama3.2 \
    --task-type nli \
    --prompt-type cot \
    --data-dir data/main_task \
    --output-dir results/main_task \
    --input-files causal.json \
    --ollama-model-name llama3.2:latest
```

---

### üìä Evaluating Results

Evaluate model predictions against gold labels:

```bash
python scripts/evaluate.py \
    --results-dir results/main_task \
    --data-dir data/main_task \
    --output-format table \
    --metric accuracy
```

#### Output Formats

- `table`: Human-readable table format
- `json`: JSON format for programmatic access
- `latex`: LaTeX table format for papers

#### Metrics

- `accuracy`: Accuracy score (correct / total)
- `f1`: F1 score (macro-averaged)

#### Examples

**Evaluate all models:**
```bash
python scripts/evaluate.py \
    --results-dir results/main_task \
    --data-dir data/main_task \
    --output-format table
```

**Evaluate specific model:**
```bash
python scripts/evaluate.py \
    --results-dir results/main_task \
    --data-dir data/main_task \
    --model gpt-4o \
    --output-format json
```

**Generate LaTeX table:**
```bash
python scripts/evaluate.py \
    --results-dir results/main_task \
    --data-dir data/main_task \
    --output-format latex \
    --metric f1
```

---

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ config/                  # Configuration files
‚îÇ   ‚îú‚îÄ‚îÄ config.yaml.example  # Example configuration template
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml          # Your API keys (gitignored)
‚îú‚îÄ‚îÄ data/                    # Input data files
‚îÇ   ‚îú‚îÄ‚îÄ main_task/          # Main task datasets
‚îÇ   ‚îú‚îÄ‚îÄ GKMRV/             # GKMRV dataset
‚îÇ   ‚îî‚îÄ‚îÄ ctrl_GKMRV/        # Control GKMRV dataset
‚îú‚îÄ‚îÄ prompts/                # Prompt templates
‚îÇ   ‚îú‚îÄ‚îÄ nli/               # NLI task prompts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ direct.txt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cot.txt
‚îÇ   ‚îî‚îÄ‚îÄ factual/           # Factual correctness prompts
‚îÇ       ‚îú‚îÄ‚îÄ direct.txt
‚îÇ       ‚îî‚îÄ‚îÄ cot.txt
‚îú‚îÄ‚îÄ src/                    # Core Python modules
‚îÇ   ‚îú‚îÄ‚îÄ models/            # LLM client implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openai_client.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gemini_client.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ollama_client.py
‚îÇ   ‚îú‚îÄ‚îÄ experiments/       # Experiment running logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ runner.py
‚îÇ   ‚îî‚îÄ‚îÄ evaluation/        # Evaluation and metrics
‚îÇ       ‚îú‚îÄ‚îÄ evaluator.py
‚îÇ       ‚îî‚îÄ‚îÄ metrics.py
‚îú‚îÄ‚îÄ scripts/               # CLI entry points
‚îÇ   ‚îú‚îÄ‚îÄ run_experiment.py  # Run experiments
‚îÇ   ‚îú‚îÄ‚îÄ evaluate.py        # Evaluate results
‚îÇ   ‚îî‚îÄ‚îÄ migrate_data.py    # Migrate old data structure
‚îú‚îÄ‚îÄ results/               # Experiment results (gitignored)
‚îÇ   ‚îî‚îÄ‚îÄ {dataset}/
‚îÇ       ‚îî‚îÄ‚îÄ {model}/
‚îÇ           ‚îî‚îÄ‚îÄ *.json
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
```

---

## ‚öôÔ∏è Configuration

### Configuration File

The main configuration file is `config/config.yaml`. See `config/config.yaml.example` for a template.

### API Key Options

You can provide API keys in three ways (in order of precedence):

1. **Command-line argument:** `--api-key YOUR_KEY`
2. **Configuration file:** `config/config.yaml`
3. **Environment variable:** `OPENAI_API_KEY`, etc.

### Azure OpenAI Configuration

For Azure OpenAI models, you need:

```yaml
lunar-{model-name}:
  api_key: "your-azure-api-key"
  version: "2024-02-15-preview"
  endpoint: "https://your-resource.openai.azure.com/"
```

---

## üìÑ Data Format

### Input Data

Input data files should be JSON with the following structure:

```json
{
  "example_id_1": {
    "premise": "Clinical information or context...",
    "statement": "Statement to evaluate...",
    "label": "entailment"  // For NLI: "entailment", "neutral", "contradiction"
                           // For factual: "True", "False"
  },
  "example_id_2": {
    ...
  }
}
```

### Results Format

Results are saved as JSON files:

```json
{
  "example_id_1": {
    "premise": "...",
    "statement": "...",
    "label": "...",
    "responses": [
      "Model response for run 1...",
      "Model response for run 2...",
      ...
    ]
  }
}
```

---

## üîÅ Reproducibility

To reproduce experiments:

1. **Set up environment:**
   ```bash
   pip install -r requirements.txt
   cp config/config.yaml.example config/config.yaml
   # Edit config.yaml with your API keys
   ```

2. **Run experiments:**
   ```bash
   python scripts/run_experiment.py \
       --model-type azure_openai \
       --model-name deepseek-r1 \
       --task-type nli \
       --prompt-type cot \
       --data-dir data/main_task \
       --output-dir results/main_task \
       --input-files causal.json comp_ground.json epistemic.json risk.json \
       --num-runs 10
   ```

3. **Evaluate results:**
   ```bash
   python scripts/evaluate.py \
       --results-dir results/main_task \
       --data-dir data/main_task \
       --output-format table
   ```

---

## üìÑ Citation

If you use this framework in your research, please cite our paper:
> **The knowledge-reasoning dissociation: Fundamental limitations of llms in clinical natural language inference**
```bibtex
@article{jullien2025knowledge,
  title={The knowledge-reasoning dissociation: Fundamental limitations of llms in clinical natural language inference},
  author={Jullien, Ma{\"e}l and Valentino, Marco and Freitas, Andr{\'e}},
  journal={arXiv preprint arXiv:2508.10777},
  year={2025}
}
```

